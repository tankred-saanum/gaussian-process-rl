import numpy as np
from matplotlib import pyplot as plt
import random
from GP_inference import *
from helper_functions import *
import NeuralDictionary


# This is an experiment testing bayesian automatic model construction on datasets generated by a (context, function)-pair
# The learner encounters a context (a feature vector) and samples from a latent function and stores the model in
# a dictionary, emulating episodic memories. These memories influence priors in later encounters with the same, or similar
# contexts. See NeuralDictionary and GP_inference for details.

# Create a set of kernels
lin = gpytorch.kernels.ScaleKernel(gpytorch.kernels.LinearKernel())
cos = gpytorch.kernels.ScaleKernel(gpytorch.kernels.CosineKernel())
rbf = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())

lin_rbf_add = lin + rbf
lin_rbf_mul = lin * rbf
lin_cos_add = lin + cos
lin_cos_mul = lin * cos
rbf_cos_add = rbf + cos
rbf_cos_mul = rbf * cos
lin_cos_rbf_add = rbf + cos + lin

core_kernels = [lin, cos, rbf, lin_cos_add, lin_rbf_add, rbf_cos_add, lin_cos_rbf_add]
parameter_arr = []

# Get kernel parameters and calculate complexity for each kernel
for kernel in core_kernels:
    param_list = []
    for param_name, value in kernel.named_hyperparameters():
        param_list.append(param_name)

    parameter_arr.append(len(param_list))
parameter_arr = np.array(parameter_arr)

# get indices and assign a prior informed by kernel complexity
indices = np.array([i for i in range(len(core_kernels))])
penalized_prior = getPenalizedUniform(parameter_arr)

# set kernel params as penalized uniform
kernel_priors = penalized_prior

# Create neural dictionary containing episodic memories:
neuro_dict = NeuralDictionary.NeuralDictionary(indices, generalization_rate=2, kernel_parameters=parameter_arr)

# Define number of context samples
context_encounters = 10
# Define range of X
sample_range = (0, 2)
# Define how many samples the agent draws at each context encounter. In a bandit task setting, this should be 1.
samples = 25

# Loop over encounters, samples of 25 are generated for each context encounter
for i in range(context_encounters):
    # create a context, i.e. get context features and a reward function
    context, features, latent_function = createContext(trial=i)
    # perform a lookup of episodic memories to inform prior
    kernel_scores = neuro_dict.look_up(features)
    # use kernel scores to get a new prior
    new_priors = neuro_dict.create_informed_prior(kernel_priors, kernel_scores)

    # Sample X, Y from latent function
    data = sampleFromContext(context, latent_function, neuro_dict, sample_range, samples)
    # unpack tuple
    action, X_norm, Y_norm, x_scale, y_scale, X, Y = data
    # Perform bayesian inference over kernel space, and create posterior predicitive distribution which integrates over
    # posterior uncertainty
    posterior = bayesianGPInference(X_norm, Y_norm, core_kernels, new_priors)

    # Append context, data and posterior to dictionary
    neuro_dict.append(context, features, posterior, X, Y)