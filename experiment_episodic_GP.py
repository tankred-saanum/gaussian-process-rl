import numpy as np
from matplotlib import pyplot as plt
import random
from GP_inference import *
from helper_functions import *
import NeuralDictionary
import gpytorch
from kernel_priors import *


float_formatter = "{:.3f}".format
np.set_printoptions(formatter={'float_kind':float_formatter})

# This is an experiment testing bayesian automatic model construction on datasets generated by a (context, function)-pair
# The learner encounters a context (a feature vector) and samples from a latent function and stores the model in
# a dictionary, emulating episodic memories. These memories influence priors in later encounters with the same, or similar
# contexts. See NeuralDictionary and GP_inference for details.

# Create a set of kernels, using priors defined in kernel_priors.py

lin = gpytorch.kernels.ScaleKernel(gpytorch.kernels.LinearKernel(variance_prior=linear_variance_prior), outputscale_prior=linear_outputscale_prior)
cos = gpytorch.kernels.ScaleKernel(gpytorch.kernels.CosineKernel(period_length_prior=per_period_len_prior), outputscale_prior=per_outputscale_prior)
rbf = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(lengthscale_prior=rbf_lengthscale_prior), outputscale_prior=rbf_outputscale_prior)

lin_rbf_add = lin + rbf
lin_rbf_mul = lin * rbf
lin_cos_add = lin + cos
lin_cos_mul = lin * cos
rbf_cos_add = rbf + cos
rbf_cos_mul = rbf * cos
lin_cos_rbf_add = rbf + cos + lin

core_kernels = [lin, cos, rbf, lin_cos_add, lin_rbf_add, rbf_cos_add, lin_cos_rbf_add]
parameter_arr = get_kernel_params(core_kernels)

# get indices and assign a prior informed by kernel complexity
indices = np.array([i for i in range(len(core_kernels))])
penalized_prior = getPenalizedUniform(parameter_arr)

# set kernel params as penalized uniform
kernel_priors = penalized_prior

# Create neural dictionary containing episodic memories:
neuro_dict = NeuralDictionary.NeuralDictionary(indices, generalization_rate=2, kernel_parameters=parameter_arr)

# Define number of context samples
context_encounters = 100
# Define range of X
sample_range = (0, 10)
sample_mean, sample_sd = 5, 3.3
test_range = (sample_range[0] - sample_sd, sample_range[1] + sample_sd)

# Define how many samples the agent draws at each context encounter. In a bandit task setting, this should be 1.
# Then define number of test values
train_samples = 10
test_samples = 100

# Loop over encounters, samples of 25 are generated for each context encounter
for i in range(context_encounters):
    # create a context, i.e. get context features and a reward function
    context, features, latent_function = createContext(trial=i)
    # perform a lookup of episodic memories to inform prior
    kernel_scores = neuro_dict.look_up(features)
    # use kernel scores to get a new prior
    new_priors = neuro_dict.create_informed_prior(kernel_priors, kernel_scores)

    # Sample X, Y from latent function
    data = sampleFromContext(context, latent_function, neuro_dict, sample_range, train_samples)
    # unpack tuple
    #action, X_norm, Y_norm, x_scale, y_scale, X, Y = data

    action, X, Y = data

    X_norm = standardize(X, sample_mean, sample_sd)
    Y_norm = Y#standardize(Y, given_mu=torch.mean(Y).item(), given_sd=1)

    test_x = create_test_x(test_range, test_samples)
    # Perform bayesian inference over kernel space, and create posterior predicitive distribution which integrates over
    # posterior uncertainty
    posterior, y_hat, upper_confidence = bayesianGPInference(X, Y, core_kernels, new_priors, test_x)


    # Append context, data and posterior to dictionary
    neuro_dict.append(context, features, posterior, X, Y)